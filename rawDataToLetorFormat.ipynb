{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################FUNCTION FOR CLEANING TEXT\n",
    "#text cleaning code\n",
    "import string\n",
    "import re\n",
    " \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    " \n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "#word U.S. and can't are getting in bad form after cleanup\n",
    "def clean_tweets(tweet,remove_stopword,remove_punctuation,remove_emoticons):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if ((word not in stopwords_english or (not remove_stopword)) and # remove stopwords\n",
    "              (word not in emoticons or (not remove_emoticons)) and # remove emoticons\n",
    "                word not in string.punctuation or (not remove_punctuation)): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################UTILITY FUNCTIONS\n",
    "# https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math\n",
    "\n",
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)\n",
    "\n",
    "def no_characters(words):\n",
    "  chars=0\n",
    "  for word in words:\n",
    "    chars=chars+len(word)\n",
    "  return chars\n",
    "\n",
    "def cosine(tweet,query):\n",
    "    vec1 = text_to_vector(tweet.lower())\n",
    "    vec2 = text_to_vector(query.lower())\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def jaccard(tweet,query):\n",
    "    vec1 = text_to_vector(tweet.lower())\n",
    "    vec2 = text_to_vector(query.lower())\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    union =set(vec1.keys()) | set(vec2.keys())\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "def dice(tweet,query):\n",
    "  vec1 = text_to_vector(tweet.lower())\n",
    "  vec2 = text_to_vector(query.lower())\n",
    "  intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "  return 2*len(intersection)/(len(vec1)+len(vec2))\n",
    "\n",
    "def count_words(tweet):\n",
    "  x=word_tokenize(tweet)\n",
    "  print(x)\n",
    "  return len(x)\n",
    "\n",
    "def get_doc(docs):\n",
    "  doc_info=[]\n",
    "  i=0\n",
    "  for doc,query in docs[['cleaned_tweet','cleaned_query']].itertuples(index=False):\n",
    "    i=i+1\n",
    "    count=len(clean_tweets(doc,False,False,True))\n",
    "    doc_info.append({'doc_id':i,'query':query,'doc_length':count})\n",
    "  return doc_info\n",
    "\n",
    "def create_freq_dict(docs):\n",
    "  freqDist_list=[]\n",
    "  i=0\n",
    "  for doc,query in docs[['cleaned_tweet','cleaned_query']].itertuples(index=False):\n",
    "    i=i+1\n",
    "    freq_dict={}\n",
    "    words=clean_tweets(doc,False,False,True)\n",
    "    for word in words:\n",
    "      if word in freq_dict:\n",
    "        freq_dict[word]+=1\n",
    "      else:\n",
    "        freq_dict[word]=1\n",
    "    freqDist_list.append({'doc_id':i,'query':query,'freq_dict':freq_dict})\n",
    "  return freqDist_list\n",
    "def computeTF(doc_info, freqDict_list):\n",
    "  TF_scores=[]\n",
    "  for tempDict in freqDict_list:\n",
    "    id=tempDict['doc_id']\n",
    "    query=tempDict['query']\n",
    "    tf_score={}\n",
    "    for key in tempDict['freq_dict']:\n",
    "      tf_score[key]=tempDict['freq_dict'][key]/doc_info[id-1]['doc_length']\n",
    "    temp={'doc_id':id,'query':query,'TF_score':tf_score}\n",
    "    TF_scores.append(temp)\n",
    "  return TF_scores\n",
    "\n",
    "def computeIDF(doc_info, freqDict_list):\n",
    "  IDF_scores=[]\n",
    "  for tempDict in freqDict_list:\n",
    "    id=tempDict['doc_id']\n",
    "    query=tempDict['query']\n",
    "    idf_score={}\n",
    "    for key in tempDict['freq_dict'].keys():\n",
    "      count=sum([key in tempDict['freq_dict'] for tempDict in freqDict_list])\n",
    "      idf_score[key]=math.log(len(doc_info)/count)\n",
    "    IDF_scores.append({'doc_id':id,'query':query,'IDF_score':idf_score})\n",
    "  return IDF_scores\n",
    "\n",
    "def TFIDF_similarity(TF_scores, IDF_scores, doc_info, freqDict_list, cosine_similarity, clean_tweets,tweets):\n",
    "  TFIDF_scores=[]\n",
    "  for tempDict in freqDict_list:\n",
    "    id=tempDict['doc_id']\n",
    "    query=tempDict['query']\n",
    "    score=0\n",
    "    sumOfSquareWeights=0\n",
    "    for key in tempDict['freq_dict']:\n",
    "      if key in query.split(\" \"):\n",
    "        tf_t_d=TF_scores[id-1]['TF_score'][key]\n",
    "        idf_t=IDF_scores[id-1]['IDF_score'][key]\n",
    "        sumOfSquareWeights=sumOfSquareWeights+idf_t**2\n",
    "        score=score+(math.sqrt(tf_t_d))*(idf_t**2)*doc_info[id-1]['doc_length']\n",
    "    coord_factor_q_d=cosine_similarity[id-1]\n",
    "    # queryNorm=1/math.sqrt(sumOfSquareWeights)#sometimes cause float division error\n",
    "    score=score*coord_factor_q_d#*queryNorm\n",
    "    TFIDF_scores.append(score)\n",
    "  return TFIDF_scores\n",
    "\n",
    "def Okapi_BM25(k,b,TF_scores, IDF_scores, doc_info, freqDict_list, cosine_similarity, tweet_lengths,clean_tweets,tweets):\n",
    "  OKAPI_scores=[]\n",
    "  average_doc_length=tweet_lengths.sum()/len(tweet_lengths)\n",
    "  for tempDict in freqDict_list:\n",
    "    id=tempDict['doc_id']\n",
    "    query=tempDict['query']\n",
    "    score=0\n",
    "    for key in tempDict['freq_dict']:\n",
    "      if key in query.split(\" \"):\n",
    "        tf_t_d=TF_scores[id-1]['TF_score'][key]\n",
    "        idf_t=IDF_scores[id-1]['IDF_score'][key]\n",
    "        numerator=(k+1)*tf_t_d\n",
    "        denominator=k*(1-b+b*(tweet_lengths[id-1]/average_doc_length))+tf_t_d\n",
    "        score=score+idf_t*(numerator/denominator)\n",
    "    OKAPI_scores.append(score)\n",
    "  return OKAPI_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Years of FAN\n",
      "Index(['Query', 'rank', 'verified', 'Img_present', 'url_bool', 'hashtag_bool',\n",
      "       'followers_count', 'friends_count', 'listed_count', 'likes', 'comments',\n",
      "       'retweets', 'sum_followers_mention', 'url_count', 'cosine', 'jaccard',\n",
      "       'hashtag_count', 'dice', 'word_count', 'char_count',\n",
      "       'follower_friend_relation', 'tfidf_similarity', 'okapi'],\n",
      "      dtype='object')\n",
      "coronavirus\n",
      "Index(['Query', 'rank', 'verified', 'Img_present', 'url_bool', 'hashtag_bool',\n",
      "       'followers_count', 'friends_count', 'listed_count', 'likes', 'comments',\n",
      "       'retweets', 'sum_followers_mention', 'url_count', 'cosine', 'jaccard',\n",
      "       'hashtag_count', 'dice', 'word_count', 'char_count',\n",
      "       'follower_friend_relation', 'tfidf_similarity', 'okapi'],\n",
      "      dtype='object')\n",
      "economical crisis\n",
      "Index(['Query', 'rank', 'verified', 'Img_present', 'url_bool', 'hashtag_bool',\n",
      "       'followers_count', 'friends_count', 'listed_count', 'likes', 'comments',\n",
      "       'retweets', 'sum_followers_mention', 'url_count', 'cosine', 'jaccard',\n",
      "       'hashtag_count', 'dice', 'word_count', 'char_count',\n",
      "       'follower_friend_relation', 'tfidf_similarity', 'okapi'],\n",
      "      dtype='object')\n",
      "INDvAUS\n",
      "Index(['Query', 'rank', 'verified', 'Img_present', 'url_bool', 'hashtag_bool',\n",
      "       'followers_count', 'friends_count', 'listed_count', 'likes', 'comments',\n",
      "       'retweets', 'sum_followers_mention', 'url_count', 'cosine', 'jaccard',\n",
      "       'hashtag_count', 'dice', 'word_count', 'char_count',\n",
      "       'follower_friend_relation', 'tfidf_similarity', 'okapi'],\n",
      "      dtype='object')\n",
      "kohli\n",
      "Index(['Query', 'rank', 'verified', 'Img_present', 'url_bool', 'hashtag_bool',\n",
      "       'followers_count', 'friends_count', 'listed_count', 'likes', 'comments',\n",
      "       'retweets', 'sum_followers_mention', 'url_count', 'cosine', 'jaccard',\n",
      "       'hashtag_count', 'dice', 'word_count', 'char_count',\n",
      "       'follower_friend_relation', 'tfidf_similarity', 'okapi'],\n",
      "      dtype='object')\n",
      "MayThe4thBeWithYou\n",
      "Index(['Query', 'rank', 'verified', 'Img_present', 'url_bool', 'hashtag_bool',\n",
      "       'followers_count', 'friends_count', 'listed_count', 'likes', 'comments',\n",
      "       'retweets', 'sum_followers_mention', 'url_count', 'cosine', 'jaccard',\n",
      "       'hashtag_count', 'dice', 'word_count', 'char_count',\n",
      "       'follower_friend_relation', 'tfidf_similarity', 'okapi'],\n",
      "      dtype='object')\n",
      "narendra modi\n",
      "Index(['Query', 'rank', 'verified', 'Img_present', 'url_bool', 'hashtag_bool',\n",
      "       'followers_count', 'friends_count', 'listed_count', 'likes', 'comments',\n",
      "       'retweets', 'sum_followers_mention', 'url_count', 'cosine', 'jaccard',\n",
      "       'hashtag_count', 'dice', 'word_count', 'char_count',\n",
      "       'follower_friend_relation', 'tfidf_similarity', 'okapi'],\n",
      "      dtype='object')\n",
      "netflix\n",
      "Index(['Query', 'rank', 'verified', 'Img_present', 'url_bool', 'hashtag_bool',\n",
      "       'followers_count', 'friends_count', 'listed_count', 'likes', 'comments',\n",
      "       'retweets', 'sum_followers_mention', 'url_count', 'cosine', 'jaccard',\n",
      "       'hashtag_count', 'dice', 'word_count', 'char_count',\n",
      "       'follower_friend_relation', 'tfidf_similarity', 'okapi'],\n",
      "      dtype='object')\n",
      "once upon a time\n",
      "Index(['Query', 'rank', 'verified', 'Img_present', 'url_bool', 'hashtag_bool',\n",
      "       'followers_count', 'friends_count', 'listed_count', 'likes', 'comments',\n",
      "       'retweets', 'sum_followers_mention', 'url_count', 'cosine', 'jaccard',\n",
      "       'hashtag_count', 'dice', 'word_count', 'char_count',\n",
      "       'follower_friend_relation', 'tfidf_similarity', 'okapi'],\n",
      "      dtype='object')\n",
      "Pandemic\n",
      "Index(['Query', 'rank', 'verified', 'Img_present', 'url_bool', 'hashtag_bool',\n",
      "       'followers_count', 'friends_count', 'listed_count', 'likes', 'comments',\n",
      "       'retweets', 'sum_followers_mention', 'url_count', 'cosine', 'jaccard',\n",
      "       'hashtag_count', 'dice', 'word_count', 'char_count',\n",
      "       'follower_friend_relation', 'tfidf_similarity', 'okapi'],\n",
      "      dtype='object')\n",
      "Silver Lake\n",
      "Index(['Query', 'rank', 'verified', 'Img_present', 'url_bool', 'hashtag_bool',\n",
      "       'followers_count', 'friends_count', 'listed_count', 'likes', 'comments',\n",
      "       'retweets', 'sum_followers_mention', 'url_count', 'cosine', 'jaccard',\n",
      "       'hashtag_count', 'dice', 'word_count', 'char_count',\n",
      "       'follower_friend_relation', 'tfidf_similarity', 'okapi'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#############CODE TO READ RAW DATA FROM FILE AND CONVERT INTO PANDAS DATAFRAME WITH COLUMNS AS FEATURES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "queries=['4 Years of FAN','coronavirus','economical crisis','INDvAUS','kohli','MayThe4thBeWithYou','narendra modi','netflix','once upon a time','Pandemic','Silver Lake']\n",
    "# queries=['artificial intelligence','womens day','pulitzer','T20 India']\n",
    "trainsets=[]\n",
    "top_n=True\n",
    "for query in queries:\n",
    "    d=pd.read_csv('data/'+query+'.csv')\n",
    "    if top_n:\n",
    "        d=d[:150]\n",
    "    print(query)\n",
    "    d['cleaned_tweet']=[\" \".join(clean_tweets(tweet,True,True,True)) for tweet in d['tweet']]\n",
    "    d['cleaned_query']=[\" \".join(clean_tweets(query,True,True,True)) for query in d['Query']]                             \n",
    "    d['rank']=[d.shape[0]-i for i in range(0,d.shape[0])]\n",
    "    if not top_n:\n",
    "        try:\n",
    "            d['how_old']=[(datetime.now()-datetime.strptime(date, '%I:%M %p · %d %b %Y')).total_seconds() for date in d['date']]\n",
    "        except:\n",
    "            d['how_old']=[(datetime.now()-datetime.strptime(date, '%I:%M %p · %b %d, %Y')).total_seconds() for date in d['date']]\n",
    "    d['cosine']=[cosine(tweet,query)*100 for tweet,query in d[['cleaned_tweet','cleaned_query']].itertuples(index=False)]\n",
    "    d['jaccard']=[jaccard(tweet,query)*100 for tweet,query in d[['cleaned_tweet','cleaned_query']].itertuples(index=False)]\n",
    "    d['url_bool']=[(0 if i==0 else 1) for i in d['url_count']]\n",
    "    d['hashtag_count']=[(0 if i is np.nan else len(i.split(','))) for i in d['tags']]\n",
    "    d['hashtag_bool']=[(0 if i==0 else 1) for i in d['hashtag_count']]\n",
    "    d['dice']=[dice(tweet,query)*100 for tweet,query in d[['cleaned_tweet','cleaned_query']].itertuples(index=False)]\n",
    "    d['word_count']=[len(tweet.split(' ')) for tweet in d['tweet']]\n",
    "    d['char_count']=[no_characters(tweet.split(' ')) for tweet in d['tweet']]\n",
    "    d['follower_friend_relation']=[0 if created_on is np.nan else 100*max(1,followers-friends)/(datetime.now()-datetime.strptime(created_on, '%Y-%m-%d %H:%M:%S')).total_seconds() for followers,friends,created_on in d[['followers_count','friends_count','created_at']].itertuples(index=False)]\n",
    "    d['follower_friend_relation']=d['word_count']\n",
    "    doc_info=get_doc(d[['cleaned_tweet','cleaned_query']])\n",
    "    freqDict_list=create_freq_dict(d[['cleaned_tweet','cleaned_query']])\n",
    "    TF_scores = computeTF(doc_info,freqDict_list)\n",
    "    IDF_scores=computeIDF(doc_info,freqDict_list)\n",
    "    TFIDF_scores=TFIDF_similarity(TF_scores,IDF_scores,doc_info,freqDict_list,d['cosine'],d['cleaned_tweet'],d['tweet'])\n",
    "    d['tfidf_similarity']=TFIDF_scores\n",
    "    OKAPI_scores=Okapi_BM25(0.75,0.5,TF_scores,IDF_scores,doc_info,freqDict_list,d['cosine'],d['word_count'],d['cleaned_tweet'],d['tweet'])\n",
    "    d['okapi']=OKAPI_scores\n",
    "    if not top_n:\n",
    "        numerical_features=d[['followers_count','friends_count','listed_count','likes','comments','retweets','sum_followers_mention','url_count','how_old','cosine','jaccard','hashtag_count','dice','word_count','char_count','follower_friend_relation','tfidf_similarity','okapi']]\n",
    "    else:\n",
    "        numerical_features=d[['followers_count','friends_count','listed_count','likes','comments','retweets','sum_followers_mention','url_count','cosine','jaccard','hashtag_count','dice','word_count','char_count','follower_friend_relation','tfidf_similarity','okapi']]\n",
    "\n",
    "#     to  scale features\n",
    "#     numerical_features=pd.DataFrame(scaler.fit_transform(numerical_features), columns=numerical_features.columns)\n",
    "    data=pd.concat([d[['Query','rank','verified','Img_present','url_bool','hashtag_bool']],numerical_features],axis=1)\n",
    "    trainsets.append(data)\n",
    "    print(data.columns) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########CODE TO WRITE THE INFO FROM DATAFRAME INTO A FILE IN FORMAT SAME AS LETOR DATASET\n",
    "file1 = open(\"MyFile150.txt\",\"w\")\n",
    "label_data = {}\n",
    "score_data = {}\n",
    "# for query in queries:\n",
    "#     label_data[query] = {}\n",
    "for index,trainset in enumerate(trainsets):\n",
    "    divider=math.ceil(len(trainset)/10)\n",
    "    for index2,row in enumerate(trainset.values):\n",
    "        s=\"\"\n",
    "        s=s+str(min(9,int(row[1]//divider)))\n",
    "        s=s+\" \"+\"qid:\"+str(index+1)\n",
    "        for j,data in enumerate(row[2:]):\n",
    "            s=s+\" \"+str(j+1)+\":\"+str(data)\n",
    "            if j+1 == 13:\n",
    "                date = data\n",
    "        s=s+\" \"+\"#docid = \"+str(index2+1)+\"_of_\"+\"_\".join(row[0].split(\" \"))\n",
    "        s=s+\"\\n\"\n",
    "        if index+1 not in label_data:\n",
    "            label_data[index+1] = {}\n",
    "            score_data[index+1] = []\n",
    "        label_data[index+1][index2+1] = min(9,int(row[1]//divider))\n",
    "        score_data[index+1].append([0,date,index2+1])\n",
    "#         print(s)\n",
    "        file1.write(s)\n",
    "#         if index2>5:\n",
    "#             break\n",
    "file1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n'\n",
      "b'Discard orig. features\\n'\n",
      "b'Model file:\\trankBoost.txt\\n'\n",
      "b'Feature normalization: zscore\\n'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rankBoostRankedLists.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9f0027b39006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mscore_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mfile1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"RankedLists.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rankBoostRankedLists.txt'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import numpy as np\n",
    "model = \"rankBoost\"\n",
    "cmd=[\"java\",\"-jar\",\"RankLib-2.13.jar\",\"-rank\",\"MyTest.txt\",\"-load\",model+\".txt\",\"-norm\",\"zscore\",\"-indri\",model+\"RankedLists.txt\"]\n",
    "output=[]\n",
    "proc = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n",
    "for line in proc.stdout.readlines():\n",
    "    print(line)\n",
    "    output.append(line[:len(line)-2])\n",
    "score_data={}\n",
    "label_data = {}\n",
    "file1 = open(\"MyTest.txt\",\"r\")\n",
    "for x in file1:\n",
    "    curr_row=x.split(\" \")\n",
    "    qid=int(curr_row[1].split(\":\")[1])\n",
    "    date=float(curr_row[14].split(\":\")[1])\n",
    "    index2=int(curr_row[26].split(\"_\")[0])-1\n",
    "    if qid not in score_data:\n",
    "        label_data[qid] = {}\n",
    "        score_data[qid] = []\n",
    "    label_data[qid][index2+1] = int(curr_row[0])\n",
    "    score_data[qid].append([0,date,index2+1])\n",
    "file1.close()\n",
    "f = open(model+\"RankedLists.txt\",\"r\")\n",
    "f1 = f.readlines()\n",
    "ans = {}\n",
    "for x in f1:\n",
    "#     print(x)\n",
    "    y = x.split()\n",
    "    value = y[len(y)-2]\n",
    "    q_id = y[0]\n",
    "    z = x.split('_')\n",
    "    i = z[0]\n",
    "    i = i.split()\n",
    "    # print(i)\n",
    "    i = i[4]\n",
    "#     print(q_id+\" \"+i+\" \"+value)\n",
    "    for v in range(len(score_data[int(q_id)])):\n",
    "        # print(score_data[int(q_id)][v][2])\n",
    "        # print(i)\n",
    "#         print(str(score_data[int(q_id)][v][2]) + \" \" + i)\n",
    "        if score_data[int(q_id)][v][2] == int(i):\n",
    "            score_data[int(q_id)][v][0] = float(value)\n",
    "#             print(\"volla\")\n",
    "            break\n",
    "for id in score_data.keys():\n",
    "    score_data[id].sort(key = lambda sub: (-sub[0], sub[1]))\n",
    "#     for x in score_data[id]:\n",
    "#         print(x[2])\n",
    "#     print(\"change\")\n",
    "# print(score_data)\n",
    "tot_ndcg = 0\n",
    "for id in score_data:\n",
    "    y_score = []\n",
    "    y_true = []\n",
    "    for doc in score_data[id]:\n",
    "        y_score.append(doc[0])\n",
    "        y_true.append(label_data[id][doc[2]])\n",
    "#         print(str(doc[0])+\" \"+str(label_data[id][doc[2]]))\n",
    "    print(ndcg(y_true,y_score))\n",
    "    tot_ndcg+=ndcg(y_true,y_score)\n",
    "tot_ndcg/=len(label_data)\n",
    "print(tot_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"lambdaMartRankedLists.txt\",\"r\")\n",
    "f1 = f.readlines()\n",
    "ans = {}\n",
    "for x in f1:\n",
    "    y = x.split()\n",
    "#     print(x)\n",
    "    value = y[len(y)-2]\n",
    "#     print(value)\n",
    "    q_id = y[0]\n",
    "    z = x.split('_')\n",
    "    i = z[0]\n",
    "    i = i.split()\n",
    "    i = i[4]\n",
    "    for v in range(len(score_data[int(q_id)])):\n",
    "        if score_data[int(q_id)][v][2] == int(i):\n",
    "            score_data[int(q_id)][v][0] = float(value)\n",
    "            break\n",
    "#     if q_id not in ans:\n",
    "#         ans[q_id] = []\n",
    "#     ans[q_id].append([float(value),int(i)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in score_data.keys():\n",
    "    score_data[id].sort(key = lambda sub: (-sub[0], sub[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg(y_true, y_score, k=1500):\n",
    "        y_true_sorted = sorted(y_true, reverse=True)\n",
    "        ideal_dcg = 0\n",
    "        for i in range(min(k,len(y_score))):\n",
    "            ideal_dcg += (2 ** y_true_sorted[i] - 1.) / np.log2(i + 2)\n",
    "        dcg = 0\n",
    "        argsort_indices = np.argsort(y_score)[::-1]\n",
    "        for i in range(min(k,len(y_score))):\n",
    "            dcg += (2 ** y_true[argsort_indices[i]] - 1.) / np.log2(i + 2)\n",
    "        ndcg = dcg / ideal_dcg\n",
    "        return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_ndcg = 0\n",
    "for id in score_data:\n",
    "    y_score = []\n",
    "    y_true = []\n",
    "    for doc in score_data[id]:\n",
    "        y_score.append(doc[0])\n",
    "        y_true.append(label_data[id][doc[2]])\n",
    "    print(ndcg(y_true,y_score))\n",
    "    tot_ndcg+=ndcg(y_true,y_score)\n",
    "tot_ndcg/=len(label_data)\n",
    "print(tot_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"lambdaMartRankedLists.txt\",\"r\")\n",
    "f1 = f.readlines()\n",
    "ans = {}\n",
    "for x in f1:\n",
    "    y = x.split()\n",
    "#     print(x)\n",
    "    value = y[len(y)-2]\n",
    "#     print(value)\n",
    "    q_id = y[0]\n",
    "    z = x.split('_')\n",
    "    i = z[0]\n",
    "    i = i.split()\n",
    "    i = i[4]\n",
    "    if q_id not in ans:\n",
    "        ans[q_id] = []\n",
    "    ans[q_id].append([float(value),int(i)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ans.keys():\n",
    "#     print(key)\n",
    "#     print(ans[key])\n",
    "#     ans[key].sort(reverse = True)\n",
    "    sorted(ans[key], key = lambda sub: (-sub[0], sub[1]))\n",
    "    #print(ans[key])\n",
    "    li = []\n",
    "    for i in ans[key]:\n",
    "#         print(label_data[int(key)][int(i[1])])\n",
    "        if int(i[1]) in label_data[int(key)]:\n",
    "            label = label_data[int(key)][int(i[1])]\n",
    "            li.append([i[1],label_data[int(key)][int(i[1])]])\n",
    "        else :\n",
    "            print(key + \" \" + i[1])\n",
    "\n",
    "    print(li) \n",
    "    print(\"change/n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = 0\n",
    "# for x in trainsets:\n",
    "#     s+=len(x)\n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# queries=['4 Years of FAN','MayThe4thBeWithYou','once upon a time','Pandemic','Silver Lake']#,'narendra modi','artificial intelligence','coronavirus','pakistan']\n",
    "# for query in queries:\n",
    "#     d=pd.read_csv('BTP/data/sample_'+query+'.csv')\n",
    "#     d=d.drop(['Unnamed: 0'], axis = 1) \n",
    "#     os.remove('BTP/data/sample_'+query+'.csv')\n",
    "#     d.to_csv('BTP/data/sample_'+query+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
