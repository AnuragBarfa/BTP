{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text cleaning code\n",
    "import string\n",
    "import re\n",
    " \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    " \n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "#word U.S. and can't are getting in bad form after cleanup\n",
    "def clean_tweets(tweet,remove_stopword,remove_punctuation,remove_emoticons):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if ((word not in stopwords_english or (not remove_stopword)) and # remove stopwords\n",
    "              (word not in emoticons or (not remove_emoticons)) and # remove emoticons\n",
    "                word not in string.punctuation or (not remove_punctuation)): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)\n",
    "\n",
    "def no_characters(words):\n",
    "  chars=0\n",
    "  for word in words:\n",
    "    chars=chars+len(word)\n",
    "  return chars\n",
    "\n",
    "def cosine(tweet,query):\n",
    "    vec1 = text_to_vector(tweet.lower())\n",
    "    vec2 = text_to_vector(query.lower())\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def jaccard(tweet,query):\n",
    "    vec1 = text_to_vector(tweet.lower())\n",
    "    vec2 = text_to_vector(query.lower())\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    union =set(vec1.keys()) | set(vec2.keys())\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "def dice(tweet,query):\n",
    "  vec1 = text_to_vector(tweet.lower())\n",
    "  vec2 = text_to_vector(query.lower())\n",
    "  intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "  return 2*len(intersection)/(len(vec1)+len(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math\n",
    "\n",
    "def count_words(tweet):\n",
    "  x=word_tokenize(tweet)\n",
    "  print(x)\n",
    "  return len(x)\n",
    "\n",
    "def get_doc(docs):\n",
    "  doc_info=[]\n",
    "  i=0\n",
    "  for doc,query in docs[['cleaned_tweet','cleaned_query']].itertuples(index=False):\n",
    "    i=i+1\n",
    "    count=len(clean_tweets(doc,False,False,True))\n",
    "    doc_info.append({'doc_id':i,'query':query,'doc_length':count})\n",
    "  return doc_info\n",
    "\n",
    "def create_freq_dict(docs):\n",
    "  freqDist_list=[]\n",
    "  i=0\n",
    "  for doc,query in docs[['cleaned_tweet','cleaned_query']].itertuples(index=False):\n",
    "    i=i+1\n",
    "    freq_dict={}\n",
    "    words=clean_tweets(doc,False,False,True)\n",
    "    for word in words:\n",
    "      if word in freq_dict:\n",
    "        freq_dict[word]+=1\n",
    "      else:\n",
    "        freq_dict[word]=1\n",
    "    freqDist_list.append({'doc_id':i,'query':query,'freq_dict':freq_dict})\n",
    "  return freqDist_list\n",
    "def computeTF(doc_info, freqDict_list):\n",
    "  TF_scores=[]\n",
    "  for tempDict in freqDict_list:\n",
    "    id=tempDict['doc_id']\n",
    "    query=tempDict['query']\n",
    "    tf_score={}\n",
    "    for key in tempDict['freq_dict']:\n",
    "      tf_score[key]=tempDict['freq_dict'][key]/doc_info[id-1]['doc_length']\n",
    "    temp={'doc_id':id,'query':query,'TF_score':tf_score}\n",
    "    TF_scores.append(temp)\n",
    "  return TF_scores\n",
    "\n",
    "def computeIDF(doc_info, freqDict_list):\n",
    "  IDF_scores=[]\n",
    "  for tempDict in freqDict_list:\n",
    "    id=tempDict['doc_id']\n",
    "    query=tempDict['query']\n",
    "    idf_score={}\n",
    "    for key in tempDict['freq_dict'].keys():\n",
    "      count=sum([key in tempDict['freq_dict'] for tempDict in freqDict_list])\n",
    "      idf_score[key]=math.log(len(doc_info)/count)\n",
    "    IDF_scores.append({'doc_id':id,'query':query,'IDF_score':idf_score})\n",
    "  return IDF_scores\n",
    "\n",
    "def TFIDF_similarity(TF_scores, IDF_scores, doc_info, freqDict_list, cosine_similarity, clean_tweets,tweets):\n",
    "  TFIDF_scores=[]\n",
    "  for tempDict in freqDict_list:\n",
    "    id=tempDict['doc_id']\n",
    "    query=tempDict['query']\n",
    "    score=0\n",
    "    sumOfSquareWeights=0\n",
    "    for key in tempDict['freq_dict']:\n",
    "      if key in query.split(\" \"):\n",
    "        tf_t_d=TF_scores[id-1]['TF_score'][key]\n",
    "        idf_t=IDF_scores[id-1]['IDF_score'][key]\n",
    "        sumOfSquareWeights=sumOfSquareWeights+idf_t**2\n",
    "        score=score+(math.sqrt(tf_t_d))*(idf_t**2)*doc_info[id-1]['doc_length']\n",
    "    coord_factor_q_d=cosine_similarity[id-1]\n",
    "    # queryNorm=1/math.sqrt(sumOfSquareWeights)#sometimes cause float division error\n",
    "    score=score*coord_factor_q_d#*queryNorm\n",
    "    TFIDF_scores.append(score)\n",
    "  return TFIDF_scores\n",
    "\n",
    "def Okapi_BM25(k,b,TF_scores, IDF_scores, doc_info, freqDict_list, cosine_similarity, tweet_lengths,clean_tweets,tweets):\n",
    "  OKAPI_scores=[]\n",
    "  average_doc_length=tweet_lengths.sum()/len(tweet_lengths)\n",
    "  for tempDict in freqDict_list:\n",
    "    id=tempDict['doc_id']\n",
    "    query=tempDict['query']\n",
    "    score=0\n",
    "    for key in tempDict['freq_dict']:\n",
    "      if key in query.split(\" \"):\n",
    "        tf_t_d=TF_scores[id-1]['TF_score'][key]\n",
    "        idf_t=IDF_scores[id-1]['IDF_score'][key]\n",
    "        numerator=(k+1)*tf_t_d\n",
    "        denominator=k*(1-b+b*(tweet_lengths[id-1]/average_doc_length))+tf_t_d\n",
    "        score=score+idf_t*(numerator/denominator)\n",
    "    OKAPI_scores.append(score)\n",
    "  return OKAPI_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Years of FAN\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data '6:57 PM · Apr 14, 2020' does not match format '%I:%M %p · %d %b %Y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-66ee51f415ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mdivider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rank'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'how_old'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%I:%M %p · %d %b %Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cosine'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_tweet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cleaned_query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'jaccard'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjaccard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_tweet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cleaned_query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-66ee51f415ea>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mdivider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rank'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'how_old'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%I:%M %p · %d %b %Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cosine'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_tweet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cleaned_query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'jaccard'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjaccard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_tweet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cleaned_query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[1;32m    564\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 565\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0;32m--> 362\u001b[0;31m                          (data_string, format))\n\u001b[0m\u001b[1;32m    363\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         raise ValueError(\"unconverted data remains: %s\" %\n",
      "\u001b[0;31mValueError\u001b[0m: time data '6:57 PM · Apr 14, 2020' does not match format '%I:%M %p · %d %b %Y'"
     ]
    }
   ],
   "source": [
    "# encoding: utf-8 \n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "queries=['4 Years of FAN','artificial intelligence','coronavirus','economical crisis','INDvAUS','kohli','MayThe4thBeWithYou','narendra modi','once upon a time','pakistan','Pandemic','Silver Lake','T20 India','womens day']\n",
    "trainsets=[]\n",
    "for query in queries:\n",
    "    print(query)\n",
    "    d=pd.read_csv('data/'+query+'.csv')\n",
    "    \n",
    "    d['cleaned_tweet']=[\" \".join(clean_tweets(tweet,True,True,True)) for tweet in d['tweet']]\n",
    "    d['cleaned_query']=[\" \".join(clean_tweets(query,True,True,True)) for query in d['Query']]                             \n",
    "    d['rank']=[d.shape[0]-i for i in range(0,d.shape[0])]\n",
    "    divider=len(d['rank'])//10\n",
    "    d['id']=[str(i)+\"_\"+d['Query'][i] for i in range(0,len(d))]\n",
    "    d['label']=[min(i//divider,9) for i in d['rank']]\n",
    "    d['how_old']=[(datetime.now()-datetime.strptime(date, '%I:%M %p · %d %b %Y')).total_seconds() for date in d['date']]\n",
    "    d['cosine']=[cosine(tweet,query)*100 for tweet,query in d[['cleaned_tweet','cleaned_query']].itertuples(index=False)]\n",
    "    d['jaccard']=[jaccard(tweet,query)*100 for tweet,query in d[['cleaned_tweet','cleaned_query']].itertuples(index=False)]\n",
    "    d['url_bool']=[(0 if i==0 else 1) for i in d['url_count']]\n",
    "    d['hashtag_count']=[(0 if i is np.nan else len(i.split(','))) for i in d['tags']]\n",
    "    d['hashtag_bool']=[(0 if i==0 else 1) for i in d['hashtag_count']]\n",
    "    d['dice']=[dice(tweet,query)*100 for tweet,query in d[['cleaned_tweet','cleaned_query']].itertuples(index=False)]\n",
    "    d['word_count']=[len(tweet.split(' ')) for tweet in d['tweet']]\n",
    "    d['char_count']=[no_characters(tweet.split(' ')) for tweet in d['tweet']]\n",
    "    d['follower_friend_relation']=[0 if created_on is np.nan else 100*max(1,followers-friends)/(datetime.now()-datetime.strptime(created_on, '%Y-%m-%d %H:%M:%S')).total_seconds() for followers,friends,created_on in d[['followers_count','friends_count','created_at']].itertuples(index=False)]    \n",
    "    doc_info=get_doc(d[['cleaned_tweet','cleaned_query']])\n",
    "    freqDict_list=create_freq_dict(d[['cleaned_tweet','cleaned_query']])\n",
    "    TF_scores = computeTF(doc_info,freqDict_list)\n",
    "    IDF_scores=computeIDF(doc_info,freqDict_list)\n",
    "    TFIDF_scores=TFIDF_similarity(TF_scores,IDF_scores,doc_info,freqDict_list,d['cosine'],d['cleaned_tweet'],d['tweet'])\n",
    "    d['tfidf_similarity']=TFIDF_scores\n",
    "    OKAPI_scores=Okapi_BM25(0.75,0.5,TF_scores,IDF_scores,doc_info,freqDict_list,d['cosine'],d['word_count'],d['cleaned_tweet'],d['tweet'])\n",
    "    d['okapi']=OKAPI_scores\n",
    "    \n",
    "    numerical_features=d[['followers_count','friends_count','listed_count','likes','comments','retweets','sum_followers_mention','url_count','how_old','cosine','jaccard','hashtag_count','dice','word_count','char_count','follower_friend_relation','tfidf_similarity','okapi']]\n",
    "    categorical_features=d[['verified','Img_present','url_bool','hashtag_bool']]\n",
    "    output_rank=d['rank']\n",
    "    output_label=d['label']\n",
    "    \n",
    "    #to  scale features\n",
    "    numerical_features=pd.DataFrame(scaler.fit_transform(numerical_features), columns=numerical_features.columns)\n",
    "    \n",
    "    data=pd.concat([d[['Query','rank','label']],categorical_features,numerical_features],axis=1)\n",
    "    trainsets.append(data)\n",
    "    print(data.columns) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"combinedData.txt\",\"w\")\n",
    "for index,trainset in enumerate(trainsets):\n",
    "    divider=math.ceil(len(trainset)/10)\n",
    "    for index2,row in enumerate(trainset.values):\n",
    "#         print(row)\n",
    "        s=\"\"\n",
    "        s=s+str(row[2])\n",
    "        s=s+\" \"+\"qid:\"+str(index+1)\n",
    "        for j,data in enumerate(row[3:]):\n",
    "            s=s+\" \"+str(j+1)+\":\"+str(data)\n",
    "        s=s+\" \"+\"#docid = \"+\"_\".join(row[0].split(\" \"))+\":\"+str(index2+1)\n",
    "        s=s+\"\\n\"\n",
    "#         print(s)\n",
    "        file1.write(s)\n",
    "file1.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
