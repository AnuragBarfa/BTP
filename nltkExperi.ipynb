{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\\\w(.)\\\\1{2,}')\n",
    "print(tweet)\n",
    "filterdText=tokenizer.tokenize(tweet)\n",
    "print(filterdText)\n",
    "def is_word_ext(word):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('([a-z])\\\\1')\n",
    "print(tweet)\n",
    "filterdText=tokenizer.tokenize(tweet)\n",
    "print(filterdText)\n",
    "def is_word_ext(word):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(tweet):\n",
    "    return sent_tokenize(tweet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(sentence):\n",
    "    return TweetTokenizer().tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.a)\n",
    "def no_word_per_tweet(tweet):\n",
    "    total_words=0\n",
    "    sentences=get_sentences(tweet)\n",
    "    for sentence in sentences:\n",
    "        print(get_words(sentence))\n",
    "        total_words=total_words+len(get_words(sentence))\n",
    "    return total_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.b)\n",
    "def no_sent_per_tweet(tweet):\n",
    "    return len(get_sentences(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.c)\n",
    "def no_word_per_sent(tweet):\n",
    "    return [len(get_words(sentence)) for sentence in get_sentences(tweet)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words as eng_dict\n",
    "low_eng_dict=[x.lower() for x in eng_dict.words()]\n",
    "#1.d)\n",
    "def freq_dict_word(words):\n",
    "    eng_word_count=0\n",
    "    print(words)\n",
    "    for word in words:\n",
    "        if word.lower() in low_eng_dict:\n",
    "            print(word)\n",
    "            eng_word_count=eng_word_count+1\n",
    "    return eng_word_count/len(words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.e)\n",
    "def freq_word_ext(word):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.f)\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.b)\n",
    "def no_punc_per_sent(sentence):\n",
    "    pun=0\n",
    "    for word in get_words(sentence):\n",
    "        if word in string.punctuation:\n",
    "            pun=pun+1\n",
    "    return pun       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.c)\n",
    "def freq_cap_words(tweet):\n",
    "    cap=0\n",
    "    for word in TweetTokenizer().tokenize(tweet):\n",
    "        if word.isupper():\n",
    "            print(word)\n",
    "            cap=cap+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_word_per_tweet(tweet)\n",
    "no_punc_per_sent(tweet)\n",
    "get_words(sent)\n",
    "lexical_diversity(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "filterdText=tokenizer.tokenize('Hello Guru99, You have build a very good site and I love visiting your site.')\n",
    "print(filterdText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('training.csv',encoding = 'utf-8',header=None)\n",
    "data.columns=['a','b','c','d','e','f']\n",
    "df = data[['e','f']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = df['e'].drop_duplicates()\n",
    "df['e'].value_counts().sort_values(ascending=False)\n",
    "dfg = df.groupby('e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dfg.get_group('lost_dog').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEY JUDE DONT BE AFRAID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet=data[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.iloc[:][[4,5]]\n",
    "data2\n",
    "data[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = casual_tokenize(tweet)\n",
    "print(sentences)\n",
    "words = []\n",
    "tokenizer = RegexpTokenizer(r'(http:\\/\\/www)')\n",
    "for sentence in sentences:\n",
    "    words.append(tokenizer.tokenize(sentence))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = nltk.Text(nltk.word_tokenize(tweet.decode('utf-8')))\n",
    "fdist = nltk.FreqDist(raw)\n",
    "print (fdist.N())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "print(words.words()[20000:21500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "'' in words.words()\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "print(tweet)\n",
    "TweetTokenizer().tokenize(tweet)\n",
    "freq_dict_word(TweetTokenizer().tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training.csv\", encoding='utf-8',header=None) \n",
    "text=data[5][0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=sent_tokenize(text)\n",
    "print(sentences)\n",
    "words=[]\n",
    "for sentence in sentences:\n",
    "    words.append(word_tokenize(sentence))    \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=RegexpTokenizer(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+] |[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|\\w+')\n",
    "token.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "tknzr.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(tweet)\n",
    "text2 = tknzr.tokenize(text)\n",
    "nltk.pos_tag(text2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
