{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)\n",
    "\n",
    "text1 = 'This is a foo bar sentence .'\n",
    "text2 = 'This sentence is similar to a foo bar sentence .'\n",
    "\n",
    "vector1 = text_to_vector(text1)\n",
    "vector2 = text_to_vector(text2)\n",
    "\n",
    "cosine = get_cosine(vector1, vector2)\n",
    "\n",
    "print('Cosine:', cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tweepy as tw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-twitter --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = twitter.Api(consumer_key='5QUCmsZc97JKVhSW7UqB4PmGO',\n",
    "                      consumer_secret='Szv1qILgFywnl1IbLakJDIqbt44unJOXWEWY7iZ2ksw6WZvDjx',\n",
    "                      access_token_key='1159073576495923200-s1eAfxDF86zf9J7brJoe2CEFwU0UW0',\n",
    "                      access_token_secret='sxA356gk2uPJFU5iIyoB2yKLUrAbnvZpSGV9cRtqzEWb7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users = api.GetFriends()\n",
    "print([u.name for u in users])\n",
    "timel=api.GetHomeTimeline()\n",
    "tie2=api.GetUserTimeline(screen_name=\"MMitanshu\")\n",
    "for ti in tie2:\n",
    "    print(ti)\n",
    "results = api.GetSearch(\n",
    "    raw_query=\"q=twitter%20&result_type=recent&since=2014-07-19&count=100\")\n",
    "for res in results:\n",
    "    print(res)\n",
    "results = api.GetSearch(raw_query=\"q=shiny pants&result_type=recent&since=2014-07-19\",result_type='mixed')\n",
    "for res in results:\n",
    "    print(res)\n",
    "#tweet_mode used to get full text inplace of truncated\n",
    "results = api.GetSearch(\n",
    "    raw_query=\"q=holi%20&tweet_mode=extended\",include_entities=True,result_type='recent')\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "url = 'https://twitter.com/search?q=shiny%20pants&src=typed_query'\n",
    "response = get(url)\n",
    "print(response.text)\n",
    "from bs4 import BeautifulSoup\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "movie_containers = html_soup.find_all('div')\n",
    "print(type(movie_containers))\n",
    "print(len(movie_containers))\n",
    "for m in movie_containers:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U selenium --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data scrapping code\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "#from selenium.common.exceptions import TimeOutException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# path where the selenium driver is copied.\n",
    "#for lab pc ashish\n",
    "#driver_path = '/home/ashishranjan/chromedriver'\n",
    "#for home pc ashish\n",
    "# driver_path = '/home/ashish/Downloads/chromedriver'\n",
    "#for lab pc anurag\n",
    "driver_path = '/home/anurag.barfa/mywork/btp/chromedriver'\n",
    "#for home pc anurag\n",
    "# driver_path = '/home/anurag/Work/btp/BTP/chromedriver'\n",
    "# This will open a new chrome session.\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--disable-notifications\")\n",
    "# https://stackoverflow.com/questions/53902507/unknown-error-session-deleted-because-of-page-crash-from-unknown-error-cannot\n",
    "chrome_options.add_argument('--no-sandbox') \n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(executable_path = driver_path,options=chrome_options)\n",
    "#driver.get(\"https://twitter.com/search?q=delhi%20election&src=typed_query\")\n",
    "query=\"test\"\n",
    "# driver.get(\"https://twitter.com/search?q=\"+query+\"&src=typed_query\")\n",
    "driver.get(\"https://twitter.com/gautamkaulgud\")\n",
    "SCROLL_PAUSE_TIME = 9\n",
    "\n",
    "# Get scroll height\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "scroll_count=10 #number of times you want to scroll the page\n",
    "tweet_boxes=[]\n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait to load page\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "    tutorial_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    tutorial_code_soup = tutorial_soup.find_all('div',attrs={'class':'css-1dbjc4n r-18u37iz r-thb0q2'})\n",
    "    tweet = tutorial_soup.find_all('div',{'class':'css-1dbjc4n r-1iusvr4 r-16y2uox r-1777fci r-5f2r5o r-1mi0q7o'})\n",
    "    tweet_boxes=tweet_boxes+tweet\n",
    "    scroll_count=scroll_count-1\n",
    "    print(scroll_count)\n",
    "    if scroll_count==0:\n",
    "        break\n",
    "            \n",
    "#tutorial_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "# tutorial_code_soup1 = tutorial_soup.find_all('div',attrs={'class':'css-1dbjc4n r-18u37iz r-thb0q2'})\n",
    "# for tweet in tutorial_code_soup1:\n",
    "#     print(tweet)\n",
    "# # tutorial_code_soup2 = tutorial_soup.find_all('ol',attrs={'id':'stream-items-id'})\n",
    "# tutorial_code_soup2 = tutorial_soup.find_all('li',attrs={'class':'js-stream-item stream-item stream-item'})\n",
    "# for tweet in tutorial_code_soup2:\n",
    "#     print(tweet)\n",
    "tweetstemp=[]\n",
    " \n",
    "for i in tweet_boxes:\n",
    "    # Add to the new list\n",
    "    # only if not present\n",
    "    if i not in tweetstemp:\n",
    "        tweetstemp.append(i)\n",
    "tweet_boxes = tweetstemp\n",
    "len(tweet_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettweet(box,query):\n",
    "    tweet=box.findChildren('div',recursive=False)[1].text\n",
    "    user=box.findChildren('div',recursive=False)[0].findChildren('div',recursive=False)[0].findChildren('div',recursive=False)[0].findChildren(recursive=False)[0].text\n",
    "    date=box.findChildren('div',recursive=False)[0].findChildren('div',recursive=False)[0].findChildren('div',recursive=False)[0].findChildren(recursive=False)[2].attrs['title']\n",
    "    likes = comments = retweet = 'None'\n",
    "    span = box.find_all('div',{'class':'css-901oao r-1awozwy r-1re7ezh r-6koalj r-1qd0xha r-a023e6 r-16dba41 r-1h0z5md r-ad9z0x r-bcqeeo r-o7ynqc r-clp7b1 r-3s2u2q r-qvutc0'})\n",
    "    likes = span[3].text\n",
    "    retweet = span[2].text\n",
    "    comments = span[1].text\n",
    "    tags=[]\n",
    "    mentions=[]\n",
    "    urls=0\n",
    "    for anchor in box.find_all('a'):\n",
    "        #print(anchor.text+\"######\")\n",
    "        if 'href' in anchor.attrs:\n",
    "            if anchor.attrs['href'].split('/')[1]=='hashtag':\n",
    "                tags.append(anchor.text)\n",
    "            if anchor.attrs['href'].split(':')[0]=='https':\n",
    "                urls=urls+1\n",
    "        if len(anchor.text)>0 and anchor.text[0]=='@':\n",
    "#             mentions.append(anchor.text)\n",
    "            if not ('aria-haspopup' in anchor.attrs and anchor.attrs['aria-haspopup']==\"false\"):\n",
    "                mentions.append(anchor.text)\n",
    "        \n",
    "    #print(tags)\n",
    "    #print(mentions)\n",
    "    cosine_similarity=cosine(tweet,query)*100\n",
    "    multimedia_flag=0\n",
    "    s = str(box.find('img'))\n",
    "    if ((box.find('img') and s.find('format')!=-1 )  or box.find('video')):\n",
    "        multimedia_flag=1  \n",
    "    return tweet,likes,comments,retweet,user,date,tags,mentions,multimedia_flag,urls,cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('data/sample_'+query+'.csv', mode='w') as sample:\n",
    "    count=0\n",
    "    sample_writer = csv.writer(sample, delimiter=',')\n",
    "    sample_writer.writerow(['Query','Date','user','Tweet','Img_present','Likes','commnets','retweet','tags','mentions','url_count','length','cosine_similarity(%)'])\n",
    "    for boxes in tweet_boxes:\n",
    "        try:\n",
    "            tweet,likes,comments,retweet,user,date,tags,mentions,multimedia_flag,urls,cosine_similarity = gettweet(boxes,query)\n",
    "            sample_writer.writerow([query,date, user, tweet, multimedia_flag,likes,comments,retweet,\",\".join(tags),\",\".join(mentions),urls,len(tweet),cosine_similarity])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gettweet(tweet_boxes[1426],query)\n",
    "try:\n",
    "#     box=tweet_boxes[1426]\n",
    "    gettweet(tweet_boxes[1426],query)\n",
    "except:\n",
    "    print(e)\n",
    "# print(box.text)\n",
    "# tweet=box.findChildren('div',recursive=False)[1].text\n",
    "# user=box.findChildren('div',recursive=False)[0].findChildren('div',recursive=False)[0].findChildren('div',recursive=False)[0].findChildren(recursive=False)[0].text\n",
    "# date=box.findChildren('div',recursive=False)[0].findChildren('div',recursive=False)[0].findChildren('div',recursive=False)[0].findChildren(recursive=False)[2].attrs['title']\n",
    "# likes = comments = retweet = 'None'\n",
    "# span = box.find_all('div',{'class':'css-901oao r-1awozwy r-1re7ezh r-6koalj r-1qd0xha r-a023e6 r-16dba41 r-1h0z5md r-ad9z0x r-bcqeeo r-o7ynqc r-clp7b1 r-3s2u2q r-qvutc0'})\n",
    "# likes = span[3].text\n",
    "# retweet = span[2].text\n",
    "# comments = span[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)\n",
    "\n",
    "def cosine(tweet,query):\n",
    "    vec1 = text_to_vector(tweet.lower())\n",
    "    vec2 = text_to_vector(query.lower())\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "#     print(intersection)\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def jaccard(tweet,query):\n",
    "    vec1 = text_to_vector(tweet.lower())\n",
    "    vec2 = text_to_vector(query.lower())\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    union =set(vec1.keys()) | set(vec2.keys())\n",
    "    return len(intersection)/len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text cleaning code\n",
    "import string\n",
    "import re\n",
    " \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    " \n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install chainer==1.17.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install tqdm --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install h5py --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rank', 'Query', 'username', 'created_at', 'verified', 'followers_count', 'friends_count', 'listed_count', 'tweet', 'date', 'Img_present', 'likes', 'comments', 'retweets', 'tags', 'mentions', 'sum_followers_mention', 'url_count']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# encoding: utf-8 \n",
    "query='INDvAUS'\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timezone\n",
    "data=[] \n",
    "training_X=[]\n",
    "training_Y=[]\n",
    "with open('data/'+query+'.csv', mode='r',encoding='utf-8') as sample:\n",
    "    sample_reader = csv.reader(sample, delimiter=',')\n",
    "    title=['rank']\n",
    "    for rank,row in enumerate(sample_reader):\n",
    "        if rank==0:\n",
    "            title=title+row\n",
    "            data=[[] for x in title]\n",
    "        else:\n",
    "            data[0].append(rank)\n",
    "            training_Y.append(rank)\n",
    "            for i in range(0,len(row)):\n",
    "                data[i+1].append(row[i])\n",
    "            how_old=(datetime.now()-datetime.strptime(row[8], '%I:%M %p · %d %b %Y')).total_seconds()\n",
    "            cos=cosine(\" \".join(clean_tweets(row[7])),\" \".join(clean_tweets(row[1])))*100\n",
    "            jac=jaccard(\" \".join(clean_tweets(row[7])),\" \".join(clean_tweets(row[1])))*100\n",
    "            training_X.append([int(row[3]), int(row[4]), int(row[5]), int(row[6]), int(how_old), int(row[9]), int(row[10]), int(row[11]), int(row[12]), int(row[15]), int(row[16]), int(cos), int(jac)])\n",
    "print(title) \n",
    "print(type(training_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.normalize(np.array(training_X))\n",
    "y = preprocessing.normalize(np.array(training_Y).reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# n=100000\n",
    "# X =  np.random.rand(n,12)\n",
    "# y =  np.random.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys,os\n",
    "\n",
    "import numpy as np\n",
    "import six\n",
    "import pickle\n",
    "import scipy\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import optimizers\n",
    "from chainer import serializers\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as ss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from learning2rank.utils import plot_result\n",
    "# -*- coding: utf-8 -*-\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "######################################################################################\n",
    "# 誤差のプロット関数。自動的に保存するので上書きされたくない時は名前を変える\n",
    "\n",
    "def acc(train_acc, test_acc, savename='result_acc.pdf'):\n",
    "    ep = np.arange(len(train_acc)) + 1\n",
    "\n",
    "    plt.plot(ep, train_acc, color=\"blue\", linewidth=1, linestyle=\"-\", label=\"Train\")\n",
    "    plt.plot(ep, test_acc, color=\"red\",  linewidth=1, linestyle=\"-\", label=\"Test\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(savename)\n",
    "    \n",
    "\n",
    "    \n",
    "def loss(train_loss, test_loss, savename='result_loss.pdf'):\n",
    "    ep = np.arange(len(train_loss)) + 1\n",
    "\n",
    "    plt.plot(ep, train_loss, color=\"blue\", linewidth=1, linestyle=\"-\", label=\"Train\")\n",
    "    plt.plot(ep, test_loss, color=\"red\",  linewidth=1, linestyle=\"-\", label=\"Test\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"loss\")\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig(savename)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class NN(object):\n",
    "\n",
    "    def loadModel(self, modelName):\n",
    "        print('Load model')\n",
    "        serializers.load_hdf5(modelName, self.model)\n",
    "        print('Load optimizer state')\n",
    "        serializers.load_hdf5(modelName[:-5] + 'state', self.optimizer)\n",
    "\n",
    "\n",
    "    def initializeModel(self, Model, train_X, n_units1, n_units2, optimizerAlgorithm):\n",
    "        print(\"prepare initialized model!\")\n",
    "        print(train_X[0])\n",
    "        self.model = Model(len(train_X[0]), n_units1, n_units2, 1)\n",
    "        self.initializeOptimizer(optimizerAlgorithm)\n",
    "\n",
    "    def initializeOptimizer(self, optimizerAlgorithm):\n",
    "        if optimizerAlgorithm == \"Adam\":\n",
    "            self.optimizer = optimizers.Adam()\n",
    "        elif optimizerAlgorithm == \"AdaGrad\":\n",
    "            self.optimizer = optimizers.AdaGrad()\n",
    "        elif optimizerAlgorithm == \"SGD\":\n",
    "            self.optimizer = optimizers.MomentumSGD()\n",
    "        else:\n",
    "            raise ValueError('could not find %s in optimizers {\"Adam\", \"AdaGrad\", \"SGD\"}' % (optimizerAlgorithm))\n",
    "        self.optimizer.setup(self.model)\n",
    "\n",
    "    def saveModels(self, savemodelName):\n",
    "        print('save the model')\n",
    "        serializers.save_hdf5(savemodelName, self.model) \n",
    "        print('save the optimizer')\n",
    "        serializers.save_hdf5(savemodelName[:-5]+ 'state', self.optimizer)  \n",
    "\n",
    "    def splitData(self, fit_X, fit_y, tv_ratio):\n",
    "        print('load dataset')\n",
    "        perm = np.random.permutation(len(fit_X))\n",
    "        N_train = int(np.floor(len(fit_X) * tv_ratio))\n",
    "        train_X, validate_X = np.split(fit_X[perm].astype(np.float32),   [N_train])\n",
    "        train_y, validate_y = np.split(fit_y[perm].astype(np.float32).reshape(len(fit_y), 1), [N_train])\n",
    "        return train_X, train_y, validate_X, validate_y\n",
    "\n",
    "    def predictTargets(self, x_pred, batchsize):\n",
    "        N_pred = len(x_pred)\n",
    "        y_pred = np.zeros(0)\n",
    "        for j in tqdm(six.moves.range(0, N_pred, batchsize)):\n",
    "            x = chainer.Variable(np.asarray(x_pred[j:j + batchsize]), volatile='on')\n",
    "            y_pred = np.append(y_pred, self.model.predict(x))\n",
    "        return y_pred\n",
    "\n",
    "    def predict(self, predict_X):\n",
    "        return self.model.predict(predict_X.astype(np.float32))\n",
    "\n",
    "    # def predict(self, predict_X, batchsize=100):\n",
    "    #     return self.predictTargets(predict_X.astype(np.float32), batchsize)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "# Define model\n",
    "class Model(chainer.Chain):\n",
    "    \"\"\"\n",
    "    ListNet - Listwise comparison of ranking.\n",
    "    The original paper:\n",
    "        http://research.microsoft.com/en-us/people/tyliu/listnet.pdf\n",
    "\n",
    "    NOTICE:\n",
    "        The top-k probability is not written.\n",
    "        This is listwise approach with neuralnets, \n",
    "        comparing two arrays by Jensen-Shannon divergence.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_units1, n_units2, n_out):\n",
    "        #in our code we have used no of features as n_in and n_out is 1\n",
    "        super(Model, self).__init__(\n",
    "            l1=L.Linear(n_in, n_units1),\n",
    "            l2=L.Linear(n_units1, n_units2),\n",
    "            l3=L.Linear(n_units2, n_out),\n",
    "        )\n",
    "\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        h1 = self.l1(x)\n",
    "        y = self.l3(F.relu(self.l2(F.relu(self.l1(x)))))\n",
    "        # self.loss = self.listwise_cost(y_data, t_data)\n",
    "        self.loss = self.jsd(t, y)\n",
    "        return self.loss\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        h = F.relu(self.l3(h2))\n",
    "        return h.data\n",
    "\n",
    "    def kld(self, vec_true, vec_compare):\n",
    "        ind = vec_true.data * vec_compare.data > 0\n",
    "        ind_var = chainer.Variable(ind)\n",
    "        include_nan = vec_true * F.log(vec_true / vec_compare)\n",
    "        z = chainer.Variable(np.zeros((len(ind), 1), dtype=np.float32))\n",
    "        # return np.nansum(vec_true * np.log(vec_true / vec_compare))\n",
    "        return F.sum(F.where(ind_var, include_nan, z))\n",
    "\n",
    "    def jsd(self, vec_true, vec_compare):\n",
    "        vec_mean = 0.5 * (vec_true + vec_compare)\n",
    "        return 0.5 * self.kld(vec_true, vec_mean) + 0.5 * self.kld(vec_compare, vec_mean)\n",
    "\n",
    "    def topkprob(self, vec, k=5):\n",
    "        vec_sort = np.sort(vec)[-1::-1]\n",
    "        topk = vec_sort[:k]\n",
    "        ary = np.arange(k)\n",
    "        return np.prod([np.exp(topk[i]) / np.sum(np.exp(topk[i:])) for i in ary])\n",
    "\n",
    "    def listwise_cost(self, list_ans, list_pred):\n",
    "        return - np.sum(self.topkprob(list_ans) * np.log(self.topkprob(list_pred)))\n",
    "\n",
    "\n",
    "class ListNet(NN):\n",
    "    \"\"\"\n",
    "    ListNet training function.\n",
    "    Usage (Initialize):\n",
    "        RankModel = ListNet()\n",
    "\n",
    "    Usage (Traininng):\n",
    "        Model.fit(X, y)\n",
    "\n",
    "    With options:\n",
    "        Model.fit(X, y, batchsize=100, n_epoch=200, n_units1=512, n_units2=128, tv_ratio=0.95, optimizerAlgorithm=\"Adam\", savefigName=\"result.pdf\", savemodelName=\"ListNet.model\"):\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, resumemodelName=None):\n",
    "        self.resumemodelName = resumemodelName\n",
    "        self.train_loss, self.test_loss = [], []\n",
    "        self.train_acc, self.test_acc = [], []\n",
    "        if resumemodelName is not None:\n",
    "            print(\"load resume model!\")\n",
    "            self.loadModel(resumemodelName)\n",
    "\n",
    "    # リストネットの誤差関数\n",
    "    def ndcg(self, y_true, y_score, k=100):\n",
    "        y_true = y_true.ravel()\n",
    "        y_score = y_score.ravel()\n",
    "        y_true_sorted = sorted(y_true, reverse=True)\n",
    "        ideal_dcg = 0\n",
    "        for i in range(k):\n",
    "            ideal_dcg += (2 ** y_true_sorted[i] - 1.) / np.log2(i + 2)\n",
    "        dcg = 0\n",
    "        argsort_indices = np.argsort(y_score)[::-1]\n",
    "        for i in range(k):\n",
    "            dcg += (2 ** y_true[argsort_indices[i]] - 1.) / np.log2(i + 2)\n",
    "        ndcg = dcg / ideal_dcg\n",
    "        return ndcg\n",
    "\n",
    "    # リストネットのトレーニング専用関数\n",
    "    def trainModel(self, x_train, y_train, x_test, y_test, n_epoch, batchsize):\n",
    "        print(\"Start training and validation loop......\")\n",
    "        N = len(x_train)\n",
    "        N_test = len(x_test)\n",
    "        for epoch in six.moves.range(1, n_epoch + 1):\n",
    "            print('epoch', epoch)           \n",
    "            # training\n",
    "            perm = np.random.permutation(N)\n",
    "            sum_loss = 0\n",
    "            for i in tqdm(six.moves.range(0, N, batchsize)):\n",
    "                x = chainer.Variable(np.asarray(x_train[perm[i:i + batchsize]]))\n",
    "                t = chainer.Variable(np.asarray(y_train[perm[i:i + batchsize]]))\n",
    "\n",
    "                self.optimizer.update(self.model, x, t)\n",
    "                sum_loss += float(self.model.loss.data) * len(t.data)\n",
    "\n",
    "            print('train mean loss={}'.format(sum_loss / N))\n",
    "            self.train_loss.append(sum_loss / N)\n",
    "\n",
    "            perm = np.random.permutation(N_test)\n",
    "            sum_loss = 0\n",
    "            for j in tqdm(six.moves.range(0, N_test, batchsize)):\n",
    "                x = chainer.Variable(np.asarray(x_test[perm[j:j + batchsize]]), volatile='off')\n",
    "                t = chainer.Variable(np.asarray(y_test[perm[j:j + batchsize]]), volatile='off')\n",
    "                loss = self.model(x, t)\n",
    "                sum_loss += float(loss.data) * len(t.data)\n",
    "            print('test  mean loss={}'.format(sum_loss / N_test))\n",
    "            self.test_loss.append(sum_loss / N_test)\n",
    "\n",
    "            train_score = self.model.predict(chainer.Variable(x_train))\n",
    "            test_score = self.model.predict(chainer.Variable(x_test))\n",
    "            train_ndcg = self.ndcg(y_train, train_score)\n",
    "            test_ndcg = self.ndcg(y_test, test_score)\n",
    "            self.train_acc.append(train_ndcg)\n",
    "            self.test_acc.append(test_ndcg)\n",
    "            print(\"epoch: {0}\".format(epoch + 1))\n",
    "            print(\"NDCG@100 | train: {0}, test: {1}\".format(train_ndcg, test_ndcg))\n",
    "\n",
    "\n",
    "    def fit(self, fit_X, fit_y, batchsize=100, n_epoch=200, n_units1=512, n_units2=128, tv_ratio=0.95, optimizerAlgorithm=\"Adam\", savefigName=\"result.pdf\", savemodelName=\"ListNet.model\"):\n",
    "        train_X, train_y, validate_X, validate_y = self.splitData(fit_X, fit_y, tv_ratio)\n",
    "        print(\"The number of data, train:\", len(train_X), \"validate:\", len(validate_X))                # トレーニングとテストのデータ数を表示\n",
    "\n",
    "        if self.resumemodelName is None:\n",
    "            self.initializeModel(Model, train_X, n_units1, n_units2, optimizerAlgorithm)\n",
    "        \n",
    "        self.trainModel(train_X, train_y, validate_X, validate_y, n_epoch, batchsize)\n",
    "\n",
    "        plot_result.acc(self.train_acc, self.test_acc)\n",
    "        plot_result.loss(self.train_loss, self.test_loss)\n",
    "        self.saveModels(savemodelName)\n",
    "\n",
    "    def test(self, fit_X, fit_y, batchsize=100, n_epoch=1, tv_ratio=0.95, optimizerAlgorithm=\"Adam\"):\n",
    "        \"\"\"\n",
    "        usage:\n",
    "        Model = ListNet(MODELNAME)\n",
    "        Model.test(fit_X, fit_y)\n",
    "        \"\"\"\n",
    "        \n",
    "        train_X, train_y, validate_X, validate_y = self.splitData(fit_X, fit_y, tv_ratio)\n",
    "        print(\"The number of data, train:\", len(train_X), \"validate:\", len(validate_X))                # トレーニングとテストのデータ数を表示        \n",
    "        self.trainModel(train_X, train_y, validate_X, validate_y, n_epoch, batchsize)\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "## end of file ##\n",
    "################################################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/362 [00:00<?, ?it/s]/home/anurag/mlenv/lib/python3.6/site-packages/chainer/functions/math/exponential.py:47: RuntimeWarning: invalid value encountered in log\n",
      "  return utils.force_array(numpy.log(x[0])),\n",
      "/home/anurag/mlenv/lib/python3.6/site-packages/ipykernel_launcher.py:147: RuntimeWarning: invalid value encountered in greater\n",
      "/home/anurag/mlenv/lib/python3.6/site-packages/chainer/functions/activation/relu.py:43: RuntimeWarning: invalid value encountered in greater\n",
      "  return utils.force_array(gy[0] * (x[0] > 0)),\n",
      "  4%|▍         | 16/362 [00:00<00:02, 150.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset\n",
      "The number of data, train: 3613 validate: 638\n",
      "prepare initialized model!\n",
      "[3.1559953e-08 1.3560258e-01 1.8651932e-05 4.2069416e-05 9.9076331e-01\n",
      " 0.0000000e+00 7.0694296e-06 3.7871945e-07 5.9963912e-07 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "Start training and validation loop......\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:01<00:00, 183.88it/s]\n",
      " 39%|███▉      | 25/64 [00:00<00:00, 241.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mean loss=-0.00022648919529443695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 217.94it/s]\n",
      "  2%|▏         | 9/362 [00:00<00:04, 85.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  mean loss=0.0\n",
      "epoch: 2\n",
      "NDCG@100 | train: 0.5099523945388695, test: 0.5135666597362101\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:01<00:00, 190.50it/s]\n",
      " 47%|████▋     | 30/64 [00:00<00:00, 298.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mean loss=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 313.02it/s]\n",
      "  0%|          | 0/362 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  mean loss=0.0\n",
      "epoch: 3\n",
      "NDCG@100 | train: 0.5099523945388695, test: 0.5135666597362101\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:01<00:00, 202.75it/s]\n",
      " 41%|████      | 26/64 [00:00<00:00, 259.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mean loss=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 297.65it/s]\n",
      "  3%|▎         | 11/362 [00:00<00:03, 107.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  mean loss=0.0\n",
      "epoch: 4\n",
      "NDCG@100 | train: 0.5099523945388695, test: 0.5135666597362101\n",
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:01<00:00, 201.90it/s]\n",
      " 44%|████▍     | 28/64 [00:00<00:00, 276.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mean loss=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 304.35it/s]\n",
      "  3%|▎         | 11/362 [00:00<00:03, 103.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  mean loss=0.0\n",
      "epoch: 5\n",
      "NDCG@100 | train: 0.5099523945388695, test: 0.5135666597362101\n",
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:01<00:00, 184.34it/s]\n",
      " 41%|████      | 26/64 [00:00<00:00, 256.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mean loss=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 295.82it/s]\n",
      "  2%|▏         | 6/362 [00:00<00:06, 56.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  mean loss=0.0\n",
      "epoch: 6\n",
      "NDCG@100 | train: 0.5099523945388695, test: 0.5135666597362101\n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:02<00:00, 176.24it/s]\n",
      " 30%|██▉       | 19/64 [00:00<00:00, 183.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mean loss=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 228.72it/s]\n",
      "  3%|▎         | 10/362 [00:00<00:03, 94.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  mean loss=0.0\n",
      "epoch: 7\n",
      "NDCG@100 | train: 0.5099523945388695, test: 0.5135666597362101\n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:02<00:00, 178.79it/s]\n",
      " 44%|████▍     | 28/64 [00:00<00:00, 269.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mean loss=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 221.84it/s]\n",
      "  3%|▎         | 10/362 [00:00<00:03, 99.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  mean loss=0.0\n",
      "epoch: 8\n",
      "NDCG@100 | train: 0.5099523945388695, test: 0.5135666597362101\n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:01<00:00, 183.27it/s]\n",
      " 28%|██▊       | 18/64 [00:00<00:00, 179.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mean loss=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 186.08it/s]\n",
      "  0%|          | 0/362 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  mean loss=0.0\n",
      "epoch: 9\n",
      "NDCG@100 | train: 0.5099523945388695, test: 0.5135666597362101\n",
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:02<00:00, 158.52it/s]\n",
      " 25%|██▌       | 16/64 [00:00<00:00, 158.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mean loss=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 220.00it/s]\n",
      "  3%|▎         | 11/362 [00:00<00:03, 103.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  mean loss=0.0\n",
      "epoch: 10\n",
      "NDCG@100 | train: 0.5099523945388695, test: 0.5135666597362101\n",
      "epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:02<00:00, 176.80it/s]\n",
      " 47%|████▋     | 30/64 [00:00<00:00, 293.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mean loss=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 313.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  mean loss=0.0\n",
      "epoch: 11\n",
      "NDCG@100 | train: 0.5099523945388695, test: 0.5135666597362101\n",
      "save the model\n",
      "save the optimizer\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "RankModel = ListNet()\n",
    "RankModel.fit(X, y, batchsize=10, n_epoch=10, n_units1=128, n_units2=128, tv_ratio=0.85, optimizerAlgorithm=\"Adam\", savefigName=\"result.pdf\", savemodelName=\"ListNet.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=RankModel.predict(X)\n",
    "for i in ans:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,len(data[4])):\n",
    "#     data[4][i]=convertToNumber(data[4][i])\n",
    "# for i in range(0,len(data[5])):\n",
    "#     data[5][i]=convertToNumber(data[5][i])\n",
    "# for i in range(0,len(data[6])):\n",
    "#     data[6][i]=convertToNumber(data[6][i])\n",
    "# for i in range(0,len(data[7])):\n",
    "#     data[7][i]=convertToNumber(data[7][i])\n",
    "# for i in range(0,len(data[10])):\n",
    "#     data[10][i]=convertToNumber(data[10][i])\n",
    "# for i in range(0,len(data[11])):\n",
    "#     data[11][i]=convertToNumber(data[11][i])\n",
    "# for i in range(0,len(data[12])):\n",
    "#     data[12][i]=convertToNumber(data[12][i])\n",
    "# for i in range(0,len(data[13])):\n",
    "#     data[13][i]=convertToNumber(data[13][i])\n",
    "# for i in range(0,len(data[16])):\n",
    "#     data[16][i]=convertToNumber(data[16][i])\n",
    "# for i in range(0,len(data[17])):\n",
    "#     data[17][i]=convertToNumber(data[17][i])\n",
    "# plt.plot(data[0], data[4])\n",
    "plt.plot(data[0], data[5])\n",
    "plt.plot(data[0], data[6])\n",
    "plt.plot(data[0], data[7])\n",
    "plt.plot(data[0], data[10])\n",
    "plt.plot(data[0], data[11])\n",
    "plt.plot(data[0], data[12])\n",
    "plt.plot(data[0], data[13])\n",
    "plt.plot(data[0], data[16])\n",
    "plt.plot(data[0], data[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tutorial_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#tweet_boxes = tutorial_soup.find_all('div',attrs={'class':'css-1dbjc4n r-18u37iz r-thb0q2','data-testid':'tweet'})\n",
    "tweet_boxes = tutorial_soup.find_all('div',{'class':'css-1dbjc4n r-1iusvr4 r-16y2uox r-1777fci r-5f2r5o r-1mi0q7o'})\n",
    "len(tweet_boxes)\n",
    "tweet_boxes[1].findChildren('div',recursive=False)[3].attrs.get('aria-label')\n",
    "for i in range(0,len(tweet_boxes)):\n",
    "    print(tweet_boxes[i].findChildren('div',recursive=False)[3].attrs.get('aria-label'))\n",
    "#     tweet_boxes[i].findChildren('div',recursive=False)[3].attrs.get('aria-label')\n",
    "box = tweet_boxes[2]\n",
    "#small_boxes = box.find_all('div',{'class':'css-901oao r-hkyrab r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0','lang':'en'})\n",
    "if(box.find('img')):\n",
    "    small_boxes = box.find('img')\n",
    "    print(small_boxes)\n",
    "boxes = tweet_boxes[2]\n",
    "#boxes.text\n",
    "#span = boxes.find_all('span',{'class':'css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0'})\n",
    "span = boxes.find_all('div',{'class':'css-901oao r-1awozwy r-1re7ezh r-6koalj r-1qd0xha r-a023e6 r-16dba41 r-1h0z5md r-ad9z0x r-bcqeeo r-o7ynqc r-clp7b1 r-3s2u2q r-qvutc0'})\n",
    "span[4].text\n",
    "#span[len(span)-1].text.isdigit()<div dir=\"ltr\" class=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "logreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(training_Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# random.shuffle(training_X)\n",
    "# x=logreg.predict(preprocessing.normalize(np.array(training_X)))\n",
    "x=logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "y=ss.rankdata(x)\n",
    "# np.array(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['rank', 'Query', 'username', 'created_at', 'verified', 'followers_count', 'friends_count', 'listed_count', 'tweet', 'date', 'Img_present', 'likes', 'comments', 'retweets', 'tags', 'mentions', 'sum_followers_mention', 'url_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in training_Y:\n",
    "#     print(i)\n",
    "ans=0;\n",
    "for i in range(0,len(y)):\n",
    "    print(y[i])\n",
    "    ans=ans+(y[i]-training_Y[i])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
